set.seed(1234)
eps = rnorm(21, 0, sd.val)
set.seed(12345)
eps.test= rnorm(21, 0, sd.val.test)
stq =sort(rep(c(0.010, 0.050, 0.100, 0.500, 1.000, 0.005, 5.000), 3))
allp = (-2.5)*log(stq)+25+eps
test1 = -2.7*log(stq)+23+eps.test
data = as.data.frame(cbind(stq, allp, test1))
data$ztest1 = (data$test1 - mean(data$test1))/sd(data$test1)
data$zallP = (data$allp - mean(data$allp))/sd(data$allp)
View(data)
covmat = as.matrix(data[, c(4,5)])
ordmod = ordinalNet(covmat, as.factor(data$stq))
summary(ordmod)
coef(ordmod, matrix=TRUE)
#kfold cv
set.seed(12)
ordfit = ordinalNetTune(covmat, as.factor(data$stq), family = "cumulative",
link = "logit", parallelTerms = TRUE, nonparallelTerms = TRUE,
warn = FALSE, printProgress = FALSE)
head(ordfit$loglik)
bestLambdaIndex = which.max(rowMeans(ordfit$loglik))
head(coef(ordfit$fit, matrix = TRUE, whichLambda = bestLambdaIndex))
#######################################
####### k-Fold Cross Validation ####### ordinalNet model
#######################################
library(caret)
n = length(calib_subset[,1])
set.seed(1)
f <- 7
folds <- rep_len(1:f, length.out = dim(calib_subset)[1])
folds <- sample(folds, size = dim(calib_subset)[1], replace = F)
table(folds)
n = length(data[,1])
set.seed(1)
f <- 7
folds <- rep_len(1:f, length.out = dim(data)[1])
folds <- sample(folds, size = dim(data)[1], replace = F)
table(folds)
preds = NULL
truth = NULL
for (k in 1:f){
test.ID = which(folds == k)
train_y = data[-test.ID, "startq"]
train_x = data[-test.ID, 2:5]
test_y = data[test.ID, "startq"]
test_x = data[test.ID, 2:5]
truth = c(truth, test_y)
model.fit = ordinalNet(as.matrix(train_x), as.factor(train_y))
preds = c(preds, predict(model.fit, test_x)) #need to fix s.t. results in 9 values rather than 45
}
for (k in 1:f){
test.ID = which(folds == k)
train_y = data[-test.ID, "startq"]
train_x = data[-test.ID, 4:5]
test_y = data[test.ID, "startq"]
test_x = data[test.ID, 4:5]
truth = c(truth, test_y)
model.fit = ordinalNet(as.matrix(train_x), as.factor(train_y))
preds = c(preds, predict(model.fit, test_x)) #need to fix s.t. results in 9 values rather than 45
}
preds = NULL
truth = NULL
for (k in 1:f){
test.ID = which(folds == k)
train_y = data[-test.ID, "stq"]
train_x = data[-test.ID, 4:5]
test_y = data[test.ID, "stq"]
test_x = data[test.ID, 4:5]
truth = c(truth, test_y)
model.fit = ordfit$fit(as.matrix(train_x), as.factor(train_y))
preds = c(preds, predict(model.fit, test_x)) #need to fix s.t. results in 9 values rather than 45
}
set.seed = (2)
expAllp.sim = rnorm(200, mean = 24.55, sd = 4.735)
set.seed = (3)
expTest1.sim = rnorm(200, mean = 18.01, sd = 2.77)
expsim = as.data.frame(cbind(expAllp.sim, expTest1.sim))
expsim$ztest1 = (expsim$expTest1.sim - mean(expsim$expTest1.sim))/sd(expsim$expTest1.sim)
expsim$zallP = (expsim$expAllp.sim - mean(expsim$expAllp.sim))/sd(expsim$expAllp.sim)
#####
#### calculating experimental starting quantity ####
k = 1
test.ID = which(folds == k)
train_y = data[-test.ID, "stq"]
train_x = data[-test.ID, 4:5]
test_y = data[test.ID, "stq"]
test_x = data[test.ID, 4:5]
truth = c(truth, test_y)
model.fit = ordfit$fit(as.matrix(train_x), as.factor(train_y))
model.fit = ordfit$fit(train_x, train_y)
train_x
train_y
model.fit = ordinalNetTune(train_x, train_y)
model.fit = ordinalNetTune(train_x, as.factor(train_y), family = 'cumulative', link = 'logit',
parallelTerms = TRUE, nonparallelTerms = TRUE, want = FALSE,
printProgress = FALSE)
100/9
library('dplyr')
library('ordinalNet')
library('precrec')
-16.05*1000
5*.2
5*.2-10*.2
0.00533/34.34
fg =c(0.00533, 0.00208, 0.00317, 0.00597, 0.00967, 0.00766)
pg = c(34.34, 35.59, 35.03, 37.25, 33.55, 33.86)
pg/fg
fg/pg
options(scipen=999)
fg/pg
x =c(10,8,13,9,11,14,6,4,12,7,5)
y =c(9.14,8.14,8.75,8.77,9.25,8.09,6.13,3.09,9.13,7.26,4.74)
Sx =sum(x)
Sy = sum(y)
Sxy = sum(x*y)
Sx2 = sum(x^2)
Sy2 = sum(y^2)
n = length(x)
numer = n*Sxy-Sx*Sy
denom = sqrt(n*Sx2-Sx^2)-sqrt(n*Sx2-Sx^2)
numer/denom
denom = sqrt(n*Sx2-Sx^2)-sqrt(n*Sy2-Sy^2)
numer/denom
sqrt(n*Sx2-Sx^2)
denom = sqrt(n*(Sx2-Sx^2))-sqrt(n*(Sy2-Sy^2)
)
denom = sqrt(n*(Sx2-Sx^2))-sqrt(n*(Sy2-Sy^2))
sqrt(n*(Sx2-Sx^2))
sqrt(11*1001-9801)
Sx^2
denom = sqrt(n*Sx2-Sx^2)-sqrt(n*Sy2-Sy^2)
n*Sxy
Sx*Sy
11*1001-9801
numer = n*Sxy-Sx*Sy
denom = sqrt(n*Sx2-Sx^2)-sqrt(n*Sy2-Sy^2)
numer/denom
denom = sqrt(n*Sx2-Sx^2)*sqrt(n*Sy2-Sy^2)
numer/denom
quiz = c(100, 90.63, 84.38, 56.25, 95, 95, 81.67, 58.33, 34.17, 100, 80.61, 92.86, 81.63, 100, 73.33,
96.3, 90.74, 57.96, 100, 89.09, 72.73, 93.33, 95.83, 79.17, 62.5, 65.28, 48.61)
exams = c(75.12, 100, 100)
final = 100
grade = mean(quiz)*.21+mean(exams)*.54+final*.25
grade
# grade calculation
quiz = c(100, 90.63, 84.38, 56.25, 95, 95, 81.67, 58.33, 34.17, 100, 80.61, 92.86, 81.63, 100, 73.33,
96.3, 90.74, 57.96, 100, 89.09, 72.73, 93.33, 95.83, 79.17, 62.5, 65.28, 48.61)
exams = c(75.12)
final = 100
grade = mean(quiz)*.21+mean(exams)*.54+final*.25
grade
.21+.18+.18+.18+.25
quiz = c(68.75, 69.7, 77.78,97.44, 87.5, 87.5, 64.37,
100, 85, 78.57,100, 86.3, 100, 83.33)
tests = c(100,57.41,  78.39)
final = 100
grade= mean(quiz)*.21+sum(.18*test)+final*.25
grade= mean(quiz)*.21+sum(.18*tests)+final*.25
grade
mean(quiz)
tests = c(90,57.41,  78.39)
final = 90
grade= mean(quiz)*.21+sum(.18*tests)+final*.25
grade
tests = c(80,57.41,  78.39)
final = 80
grade= mean(quiz)*.21+sum(.18*tests)+final*.25
grade
mean(quiz)
(57.41+87.39)/2
tests = c(100,57.41,  78.39)
final = 100
grade= mean(quiz)*.21+sum(.18*tests)+final*.25
grade
quiz = c(80.3, 87.5, 94.44, 100, 100, 97.5, 96.55, 100, 100, 100, 100, 100, 91.67, 0, 0)
tests = c(100,57.14,  92.5)
final = 100
grade= mean(quiz)*.21+sum(.18*tests)+final*.25
grade
mean(quiz)
18*3
Nv = 292
Nl = 292
Nt = Nv+Nl
Pv = 0.61
Pl = 0.08
Pt = ((Pv*Nv)+(Pl*Nl))/Nt
Z = (Pv-Pl)/sqrt(Pt*(1-Pt)*(2/Nv))
Z
(0.61-0.08)/sqrt((0.35*(1-0.35)*(2/292))
)
library(stringr)
library(tidyverse)
library(dplyr)
setwd("C:/Users/twili/Desktop/GIThub/Andrew/stapleton_lab/Stress_Splicing/SamplingPlan")
#############################################
##### formatting the Sampling Plan data #####
#############################################
dat = read.csv(file = "../2016_Clayton/Field Book (2016) - Clayton - Sampling Plan_TIDIED.csv", header = TRUE)
#filter out the recongizable MO###
dat = dat %>% filter(str_detect(dat$Genotype, "Mo") == TRUE)
#create the breedtype category
BreedType = ifelse(substr(dat$Genotype, 1,1)=="M", "Inbred", "Hybrid")
dat = cbind(BreedType, dat)
snpFull = read.csv(file = "../IBM94markerset08seq.csv", header = TRUE)
snp = snpFull[,-(1:5)]
#############################################
### matching the Mo### to their snp Values ##
#############################################
library(data.table)
#function takes the last n characters of a string
substrRight <- function(x, n){
substr(x, nchar(x)-n+1, nchar(x))
}
#creating datasets exclusively containing the last three numbers
dat.num =cbind(dat, as.integer(substrRight(as.character(dat$Genotype), 3)))
#produces warning sign about NA's for data that is not availible for invalid Mo###
snp.num = as.integer(substrRight(as.character(colnames(snp)), 3))
snp.new = data.frame(lapply(snp,as.character),stringsAsFactors=FALSE)
snpMatch = rbind(snp.num,snp.new)
snpMatch = transpose(snpMatch)
# creating matching Genotype columns to merge the data
#need = c(1,5,7,8,9,10,16) #column numbers of needed variables
#dat.num = dat.num[,need]
colnames(snpMatch) = "GenotypeNum"
#datnames =names(dat.num)
colnames(dat.num)[17] = "GenotypeNum"
dat2 = merge(dat.num,snpMatch, by.x = "GenotypeNum", by.y = "GenotypeNum",all.MoNum = all)
dat2 = dat2[order(dat2$Genotype, decreasing = FALSE),]
dat2 = dat2[,-c(1, 4)] #omit GenotypeNum
dat2[1:20, 1:20]
#write.csv(dat2, "SamplingPlan_dat2.csv")
########################################
######### Including plate data #########
########################################
colnames(dat2)[4] = "sampleID.exp"
# ### by month framing ####
#
# # 2018_11 plate data #
# ### the file in plate will come from the qPCR output including stress ratios ###
# plate_11 = read.csv(file = "../2018_11/2018_11_withStress.csv", header = TRUE)
# colnames(plate_11)[2] = "sampleID"
# full_11 = merge(plate_11, dat2, by = "sampleID")
#
# # 2018_6 plate data #
# plate_6 = read.csv(file = , header = TRUE)
# colnames(plate_6)[2] = "sampleID"
# full_6 = merge(plate_6, dat2, by = "sampleID")
#
# # 2018_8 plate data #
# plate_8 = read.csv(file = , header = TRUE)
# colnames(plate_8)[2] = "sampleID"
# full_8 = merge(plate_8, dat2, by = "sampleID")
#
# #create one big dataframe containing all plate months
# #full = rbind(full_11, full_6, full_8)
#
# #### Heirarchical Model Framing ####
HPlate = read.csv(file = "../Heirarchical/Hierarchical_exp_data_stress.csv")
#split isolate the June month since the sampleID is availible for it
June = HPlate %>% filter(str_detect(HPlate$month, "june") == TRUE)
colnames(June)[2] = "Barcode"
June = merge(June, dat2, by = "Barcode")
#August and November Months
AugNov = HPlate %>% filter(str_detect(HPlate$month, "aug")|str_detect(HPlate$month, "nov"))
AugNov = merge(AugNov, dat2, by = "sampleID.exp")
#making sure that non of the genotype information was matched to more than on obs
bad =intersect(June$Barcode, AugNov$Barcode)
length(bad)
baddoup = AugNov[match(bad, AugNov$Barcode),1:20]
repJune = June[c(21,32),] # we want to replace the repeated values in June with N/A
June[c(21,32), ] = NA
full = rbind(June, AugNov)
head(names(full), 20)
full = cbind(full$stress, full[,-12])
colnames(full)[1] = "stress"
#####Adding marker location and chromosome#####
addmarker <- function(full, plate){
aux = matrix(snpFull$incre_new, nrow= 1)
aux = rbind(snpFull$Chromosome, aux)
zeros = dim(full)[2]-dim(aux)[2]
fillnames = names(full)[1:zeros]
other = as.data.frame(matrix(rep(0,2*zeros), nrow = 2)) #repeat the number of 0 as the number of variables
aux = cbind(other,aux)
colnames(aux) = rep("",dim(aux)[2])
colnames(full) = rep("",dim(aux)[2])
dat3 = rbind(aux,full)
colnames(dat3) = c(fillnames, as.character(snpFull$markername))
return(dat3)
}
#make sure to delete extra 0's and also delete '('
full = addmarker(full)
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 20) #BreedType = index 13
#unnecessary columns for vQTL input, we only need: Stress, breedtype, genotype, barcode
notneed = c(2:7,9:11,14:17,19:26)
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 20) #BreedType = index 13
full = full[,-notneed]
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 20) #BreedType = index 13
#make sure to delete extra 0's and also delete '('
full = addmarker(full)
full = rbind(June, AugNov)
#moving stress to the first column
head(names(full), 20)
full = cbind(full$stress, full[,-12])
colnames(full)[1] = "stress"
addmarker <- function(full, plate){
aux = matrix(snpFull$incre_new, nrow= 1)
aux = rbind(snpFull$Chromosome, aux)
zeros = dim(full)[2]-dim(aux)[2]
fillnames = names(full)[1:zeros]
other = as.data.frame(matrix(rep(0,2*zeros), nrow = 2)) #repeat the number of 0 as the number of variables
aux = cbind(other,aux)
colnames(aux) = rep("",dim(aux)[2])
colnames(full) = rep("",dim(aux)[2])
dat3 = rbind(aux,full)
colnames(dat3) = c(fillnames, as.character(snpFull$markername))
return(dat3)
}
#make sure to delete extra 0's and also delete '('
full = addmarker(full)
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 20) #BreedType = index 13
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 25) #BreedType = index 13
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 30) #BreedType = index 13
#unnecessary columns for vQTL input, we only need: Stress, breedtype, genotype, barcode
notneed = c(2:7,9:12,14:17,19:26)
full = full[,-notneed]
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 30) #BreedType = index 13
full = rbind(June, AugNov)
#moving stress to the first column
head(names(full), 20)
full = cbind(full$stress, full[,-12])
colnames(full)[1] = "stress"
addmarker <- function(full, plate){
aux = matrix(snpFull$incre_new, nrow= 1)
aux = rbind(snpFull$Chromosome, aux)
zeros = dim(full)[2]-dim(aux)[2]
fillnames = names(full)[1:zeros]
other = as.data.frame(matrix(rep(0,2*zeros), nrow = 2)) #repeat the number of 0 as the number of variables
aux = cbind(other,aux)
colnames(aux) = rep("",dim(aux)[2])
colnames(full) = rep("",dim(aux)[2])
dat3 = rbind(aux,full)
colnames(dat3) = c(fillnames, as.character(snpFull$markername))
return(dat3)
}
#make sure to delete extra 0's and also delete '('
full = addmarker(full)
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 30) #BreedType = index 13
#unnecessary columns for vQTL input, we only need: Stress, breedtype, genotype, barcode
notneed = c(3:7,9:12,14:17,19:26)
full = full[,-notneed]
#####MAKE SURE TO DELETE THE EXTRA ZEROS IN [1:2,1:4] IN EXCEL AFTERWARDS#####
head(names(full), 30) #BreedType = index 13
write.csv(full, file = "../Heirarchical/vqtlinput.csv" ,row.names = FALSE)
#needed blank space for vqtl
blanksp = full[1:2,]
FullInb = full %>% filter(str_detect(full$BreedType, "Inbred") == TRUE)
FullHyb = full %>% filter(str_detect(full$BreedType, "Hybrid") == TRUE)
write.csv(rbind(blanksp,FullInb), file = "../Heirarchical/FullInbvqtlinput.csv" ,row.names = FALSE)
write.csv(rbind(blanksp,FullHyb), file = "../Heirarchical/FullHybvqtlinput.csv" ,row.names = FALSE)
library(MASS)
library(stringr)
## Ordinal Net package ##
library("ordinalNet")
# set directory to the Heirarchical folder
setwd("C:/Users/twili/Desktop/GIThub/Andrew/stapleton_lab/Stress_Splicing/Heirarchical")
#### reading in and setting up calibrated data ####
# MONTH 1 (2018_6 / JUNE) CALIBRATED DATA FRAME
calib_data_6 = na.omit(read.csv("../2018_6/calib_2018_6.csv")[,-1])
calib_data_6$ztest1 = (calib_data_6$test1 - mean(calib_data_6$test1))/sd(calib_data_6$test1)
calib_data_6$zallP = (calib_data_6$allP - mean(calib_data_6$allP))/sd(calib_data_6$allP)
calib_data_6$month ='june'
#calib_data_6
# MONTH 2 (2018_8 / AUGUST) CALIBRATED DATA FRAME
calib_data_8 = na.omit(read.csv("../2018_8/calib_2018_8.csv")[,-1])
calib_data_8$ztest1 = (calib_data_8$test1 - mean(calib_data_8$test1))/sd(calib_data_8$test1)
calib_data_8$zallP = (calib_data_8$allP - mean(calib_data_8$allP))/sd(calib_data_8$allP)
calib_data_8$month ='aug'
#calib_data_8
# MONTH 3 (2018_11 / NOVEMBER) CALIBRATED DATA FRAME
calib_data_11 = na.omit(read.csv("../2018_11/calib_2018_11.csv")[,-1])
calib_data_11$ztest1 = (calib_data_11$test1 - mean(calib_data_11$test1))/sd(calib_data_11$test1)
calib_data_11$zallP = (calib_data_11$allP - mean(calib_data_11$allP))/sd(calib_data_11$allP)
calib_data_11$month ='nov'
#calib_data_11
# Combined Calib d.f. for all months
calib_data = rbind(calib_data_6, calib_data_8, calib_data_11)
# Create dummy varible columns for each month
calib_data$june = ifelse(str_detect(calib_data[,6], "june"), 1, 0)
calib_data$aug = ifelse(str_detect(calib_data[,6], "aug"), 1, 0)
#calib_data = calib_data[,-6]
# create a subset only containing the startq, zvalues, and dummy months
calib_subset = calib_data[,c(1, 4,5,8,7)]
######
#### Plotting calibrated data ####
#graphing log starting quantity to cp values
plot(calib_data$allP,log(calib_data$startq), col = 'red', main = "Hierarchical Log Starting Quanitty vs. Cp Values",
ylab = "log(Starting Quantity)", xlab = "Cp Values" , xlim = c(5, 25), ylim = c(-6, 1))
points(calib_data$test1,log(calib_data$startq), col = 'blue')
abline(lm(log(calib_data$startq)~calib_data$allP), col = 'red')
abline(lm(log(calib_data$startq)~calib_data$test1), col = 'blue')
legend('topright', legend=c("Test 1", "All Products"),
col=c("blue", "red"), lty = 1, cex=0.8)
#define ordinal model starq~zallP+ztest1+month
ordmod = ordinalNet(as.matrix(calib_subset[,2:5]), as.factor(calib_subset$startq))
summary(ordmod)
coef(ordmod, matrix=TRUE)
#kfold cv
set.seed(3)
ordfit = ordinalNetTune(as.matrix(calib_subset[,2:5]), as.factor(calib_subset$startq), family = "cumulative",
link = "logit", parallelTerms = TRUE, nonparallelTerms = TRUE,
warn = FALSE, printProgress = FALSE)
head(ordfit$loglik)
bestLambdaIndex = which.max(rowMeans(ordfit$loglik))
head(coef(ordfit$fit, matrix = TRUE, whichLambda = bestLambdaIndex))
# MONTH 1 (2018_8 / AUGUST) EXPERIMENTAL DATA FRAME
exp_data_8 = na.omit(read.csv("../2018_8/exp_2018_8.csv")[,-1])
exp_data_8$ztest1 = (exp_data_8$test1 - mean(exp_data_8$test1))/sd(exp_data_8$test1)
exp_data_8$zallP = (exp_data_8$allP - mean(exp_data_8$allP))/sd(exp_data_8$allP)
exp_data_8$month ='aug'
exp_data_8$sampleID.exp = as.factor(exp_data_8$sampleID.exp)
# MONTH 2 (2018_6 / JUNE) EXPERIMENTAL DATA FRAME
exp_data_6 = na.omit(read.csv("../2018_6/exp_2018_6.csv")[,-c(1,5,6)])
exp_data_6$ztest1 = (exp_data_6$test1 - mean(exp_data_6$test1))/sd(exp_data_6$test1)
exp_data_6$zallP = (exp_data_6$allP - mean(exp_data_6$allP))/sd(exp_data_6$allP)
exp_data_6$month ='june'
# MONTH 3 (2018_11 / NOVEMBER) EXPERIMENTAL DATA FRAME
exp_data_11 = na.omit(read.csv("../2018_11/exp_2018_11.csv")[,-1])
exp_data_11$ztest1 = (exp_data_11$test1 - mean(exp_data_11$test1))/sd(exp_data_11$test1)
exp_data_11$zallP = (exp_data_11$allP - mean(exp_data_11$allP))/sd(exp_data_11$allP)
exp_data_11$month ='nov'
exp_data_11$sampleID.exp = as.factor(exp_data_11$sampleID.exp)
# Combined exp d.f. for all months
exp_data = rbind(exp_data_6, exp_data_8, exp_data_11)
# Create dummy varible columns for each month
exp_data$june = ifelse(str_detect(exp_data[,6], "june"), 1, 0)
exp_data$aug = ifelse(str_detect(exp_data[,6], "aug"), 1, 0)
# Drop rows containing NA
exp_subset = exp_data[,c(1, 4, 5, 7, 8)]
#### calculating experimental starting quantity ####
probmat = predict(ordfit$fit, as.matrix(exp_subset[,2:5]))
probmat[1:10,]
group = split.data.frame(calib_data, calib_data$startq)
adj <- function(AllP, Test1){
adjust = ave(AllP)-ave(Test1)
return(adjust)
}
adjval = NULL
for (k in group){
adjval = c(adjval,adj(k$allP, k$test1))
}
##### Creating a dataframe with the stq and adjustment values #########
calib_adj = as.data.frame(cbind(unique(calib_data$startq), unique(adjval)))
colnames(calib_adj) = c("startq", "adj")
#convert adjustment from picograms to femtograms (1:1000)
calib_adj$adj = (calib_adj$adj)*1000
# convert test1 and allp to femtograms
exp_data[,c(2,3)] = exp_data[,c(2,3)]*1000
# Apply probability matrix to the adjustment values using matrix multiplication
exp_data$exp.adjust = probmat%*%calib_adj$adj
# Create new column with stress product (VQTL input)
exp_data$exp.adjustTest1 = exp_data$test1.exp+exp_data$exp.adjust
# convert allP and adjusted test 1 to femtograms
exp_data$stress = exp_data$allP.exp - exp_data$exp.adjustTest1
boxplot(exp_data$allP.exp, exp_data$test1.exp, exp_data$stress, main = "Boxplot of Experimental All Products, Test 1, and Stress",
names =c("All Products", "Test 1", "Stress"), ylab = "Cp value", col =c("blue", "red", "green"))
hist(exp_data$stress, col = "light blue")
### analyzing negative stress ###
negstress = na.omit(ifelse(exp_data$stress<0,exp_data$stress,NA))
hist(negstress, col = "light green")
length(negstress)/length(exp_data$stress)
length(negstress)
length(exp_data$stress)
summary(negstress)
View(exp_data)
negstress
?na.omit()
ifelse(exp_data$stress<0,exp_data$stress == NA)
### analyzing negative stress ###
negstress = ifelse(exp_data$stress<0,exp_data$stress == NA, )
### analyzing negative stress ###
negstress = ifelse(exp_data$stress<0,exp_data$stress == NA, exp_data$stress == exp_data$stress)
negstress
### analyzing negative stress ###
negstress = ifelse(exp_data$stress<0,exp_data$stress = NA, exp_data$stress = exp_data$stress)
### analyzing negative stress ###
negstress = exp_data %>% filter(exp_data$stress<0)
View(negstress)
hist(negstress$stress, col = "light green")
length(negstress)/length(exp_data$stress)
length(negstress$stress)/length(exp_data$stress)
# making negstress "NA" #
na.obs = match(negstress$stress,exp_stress$stress)
# making negstress "NA" #
na.obs = match(negstress$stress,exp_data$stress)
na.obs
# we want to replace the negative values with N/A
exp_data[na.obs, ] = NA
min(exp_data$stress)
hist(exp_data$stress)
### Write the exp data with the stress product as a new data frame for vqtl matching
write.csv(exp_data, "Hierarchical_exp_data_stress.csv")
